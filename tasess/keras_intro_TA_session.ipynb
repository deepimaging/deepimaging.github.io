{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "keras_intro_TA_session.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-uEbvk2_g7n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get tensorflow 2.0 (run once per session)\n",
        "pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "982OTw7I_uxc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print('tensorflow version: ' + tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIYgWeQl_6SS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load mnist dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # customary normalization to [0, 1]\n",
        "\n",
        "# because we're using CNNs, the data needs a channel dimension:\n",
        "x_train = x_train[..., None]\n",
        "x_test = x_test[..., None]\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fczsbMgHNCS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model = tf.keras.models.Sequential([\n",
        "    # let's add some convolutional layers:\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # now, let's transition into a fully-connected layer; first, we flatten:\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(units=128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # length-10 output for classification\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam',  # pick an optimizer\n",
        "                     loss='sparse_categorical_crossentropy',  # pick a loss\n",
        "                     metrics=['accuracy'])  # pick a metric to monitor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ijRtGnrI51V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model and track validation loss after each epoch:\n",
        "cnn_model.fit(x_train, y_train,\n",
        "              epochs=5,\n",
        "              batch_size=32,\n",
        "              validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veYIa9O0e3eX",
        "colab_type": "text"
      },
      "source": [
        "## Implementing custom layers\n",
        "Although keras provides all the most popular neural network layers (e.g., Conv2D, BatchNormalization, etc.), as a researcher who wants to develop new architectures, you may want to define a custom layers based on low-level operations. In this exercise, we will create a custom layer that implements Dense (or fully-connected layer), which is also useful for understanding what is happening under the hood that keras has abstracted away. See https://www.tensorflow.org/guide/keras/custom_layers_and_models for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJzBEt3QVUxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fc(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_outputs, activation):\n",
        "    super().__init__()  # invoke the super class\n",
        "    # constructor, which handles things like dtype, name,\n",
        "    # training-related stuff, ...\n",
        "    self.num_outputs = num_outputs\n",
        "    self.activation = activation\n",
        "    \n",
        "  def build(self, input_shape):\n",
        "    # if we want to make this layer type general for any input shape, then we\n",
        "    # can't define the variables until we know the input shape, which is passed\n",
        "    # to the build function, which we define here, which is called by the call\n",
        "    # function below;\n",
        "    # input shape is flattened, given by [batch_size, flattened data]\n",
        "    self.W = tf.Variable(initial_value=\n",
        "                         np.random.randn(input_shape[1],\n",
        "                                    self.num_outputs).astype(np.float32))\n",
        "    self.b = tf.Variable(initial_value=\n",
        "                         np.random.randn(self.num_outputs).astype(np.float32))\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    return self.activation(tf.matmul(inputs, self.W) + self.b[None])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uhPX91Xmg1E",
        "colab_type": "text"
      },
      "source": [
        "Now, let's repeat the above model, using our own fc instead of the built-in Dense."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJT8Ejo2me-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cnn_model = tf.keras.models.Sequential([\n",
        "    # let's add some convolutional layers:\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding='same', activation='relu'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    # now, let's transition into a fully-connected layer; first, we flatten:\n",
        "    tf.keras.layers.Flatten(),\n",
        "    fc(num_outputs=128, activation=tf.nn.relu),\n",
        "    fc(num_outputs=10, activation=tf.nn.softmax)\n",
        "])\n",
        "\n",
        "cnn_model.compile(optimizer='adam',  # pick an optimizer\n",
        "                     loss='sparse_categorical_crossentropy',  # pick a loss\n",
        "                     metrics=['accuracy'])  # pick a metric to monitor\n",
        "# train model and track validation loss after each epoch:\n",
        "cnn_model.fit(x_train, y_train,\n",
        "              epochs=5,\n",
        "              batch_size=32,\n",
        "              validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nY8gLmIzk9g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}