<!-- The structure for this template is code adapted from https://cs.nyu.edu/~deigen/depth/ -->
<!-- Please site this website if made public -->
<HTML>
<HEAD>
<title>Gesture Recognition on Low Resolution American Sign Language (ASL) Images</title>
<LINK REL="stylesheet" HREF="style.css">
</HEAD>
<BODY bgcolor="white">
<center>
<h2>Gesture Recognition on Low Resolution American Sign Language (ASL) Images</h2>
<h3>Altaful Amin &nbsp;&nbsp;&nbsp;&nbsp;
    
</h3>
<h4>
aa547<!-- x -->@<!-- -->duke.<!-- h -->edu
&nbsp;&nbsp;&nbsp;
</h4>
<p>
<table border=0 width="25%">
<tr>
<!-- Provide link to your paper below -->
<td align="center"><a href="Final_Report_Altaful_Amin.pdf"><font size="+1">Paper PDF</font></a>
</tr>
</table>

<!-- Provide link to your "teaser figure - this should summarize your findings at a glace" -->
<p>
<img src="teaser_image.png" width="60%">
<p>
<table width="80%">
<tr><td align="left">
<p>
American Sign Language is widely used among the Deaf communities in the US and Canada. A machine learning system that can take an image of the sign being made and interpret it for other non-users would be very helpful to bridge the gap between ASL users and non-users without an interpreter. In this project, I developed a CNN model that will train on images of the ASL letters of the alphabet. I also simulated lower resolution images to see how well the model performs on different sizes of image, keeping in mind an end goal of developing a model that can be used by a wide range of image capturing devices. Results showed that the model performed at 89% accuracy and higher on the image datasets.  


<br>

<!-- Provide link to your write-up -->
<tr><td align="left">
Paper:
<ul>
<li><a href="Final_Report_Altaful_Amin.pdf"><font size="+1">Paper PDF</font></a>
</ul>

<tr><td align="left">
Data:
<ul>
<li>Sign Language MNIST: <a href="https://www.kaggle.com/datamunge/sign-language-mnist">Image Dataset</a>
</ul>

</table>
</center>



</BODY>
