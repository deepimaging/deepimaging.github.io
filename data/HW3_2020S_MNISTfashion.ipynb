{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4SOHrmeF9HJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get tensorflow 2.0 (run once per session)\n",
        "!pip install tensorflow==2.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phIROnd7F_JQ",
        "colab_type": "code",
        "outputId": "85f0449a-480c-4088-cee8-a9b681be824e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# customary imports:\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print('tensorflow version: ' + tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensorflow version: 2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNv61VDIHDgb",
        "colab_type": "text"
      },
      "source": [
        "## 1. Load fashion MNIST dataset from tf.keras.datasets\n",
        "Use the tf.keras.datasets module to load the fashion MNIST dataset. Please normalize the dataset to range from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O4LB0cdDHBrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load fashion_mnist dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0  # customary normalization to [0, 1]\n",
        "\n",
        "# because we're using CNNs, the data needs a channel dimension:\n",
        "x_train = x_train[..., None]\n",
        "x_test = x_test[..., None]\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBvySu7UqbhC",
        "colab_type": "text"
      },
      "source": [
        "## 2. Define a keras sequential model\n",
        "(a) Please define your sequential model with the following layers:\n",
        "1. A convolutional layer with a 5x5 kernel and stride of 1\n",
        "2. A convolutional layer with a 5x5 kernel and stride of 1\n",
        "3. A pooling layer (Instead of this, you could also increase the stride in the second layer)\n",
        "4. A convolutional layer with a 5x5 kernel and stride of 1\n",
        "5. A convolutional layer with a 5x5 kernel and stride of 1\n",
        "6. A pooling layer (Instead of this, you could also increase the stride in the fifth layer)\n",
        "7. A Dense layer\n",
        "8. Output layer of size 10\n",
        "\n",
        "You are free to choose the sizes, number of channels and activations (i.e., the employed non-linearity) for each of the layers.\n",
        "\n",
        "(b) Now, please comment out the pooling layer in step 3 and step 6, and instead increase the stride in the appropriate layers to achieve the same down-sampling effect (i.e., to reduce the size of the datacube in the same way as pooling) \n",
        "\n",
        "(c) After defining the Sequential model, you should define an optimizer and set a learning rate. You also should pick a loss function.\n",
        "\n",
        "(d) Run the optimization for 10-15 epochs and monitor the training and validation loss and accuracy. After training is done, please plot two graphs, one showing the training and validation losses as two curves within the same plot, and a second graph that shows the the training and validation accuracies as two curves within the same plot. For both plots, please let epoch be the horizontal axis.\n",
        "\n",
        "At the end of training, you should be able obtain an accuracy better than 85%.\n",
        "\n",
        "You may refer to the notebook from the TA session or any online TensorFlow resources for guidance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdaCeAafKfBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5t_4Q3Zpcbm",
        "colab_type": "text"
      },
      "source": [
        "## 3. How many weight parameters does your network have?\n",
        "First try calculating this number by hand, and show your work (please type out the multiplications that you are performing to arrive at the final number.) Then, please verify the answer using Keras's autogenerated model summary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAGKahMOtgq2",
        "colab_type": "text"
      },
      "source": [
        "## 4. Visualise filters\n",
        "You can obtain weights in individual layers by running \n",
        "```\n",
        "your_model_variable.layers[layer_index].get_weights()\n",
        "```\n",
        "(a) Plot all convolution kernels (i.e., each set of 5x5 weights) in your first convolutional layer.\n",
        "\n",
        "(b) What is the variance of final weights in the first convolutional layer? \n",
        "\n",
        "(c) Also plot some of the convolutional weights in the second layer. What is the variance of the final weights in the second layer? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJmhDicQKlga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIzRfmIkq-A0",
        "colab_type": "text"
      },
      "source": [
        "## 5. Try playing with the learning rate\n",
        "(a) Try to increase and decrease the learning rate and plot the training and validation loss and accuracy curves from part 2(d), for three different values of learning rate that you have tried. \n",
        "\n",
        "(b) Please comment on any trends that you can identify between how the plots change as a function of learning rate. Specifically, what happens to the slopes of the training loss and accuracy as a function of learning rate?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DDZbZ58KpSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d16axQgDqZEk",
        "colab_type": "text"
      },
      "source": [
        "## 6. Adding Batch Norm\n",
        "Fix a value of the learning rate and try adding Batch Normalization after layers 2 and 5. Does it improve the performance of your model? Explain briefly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8LDjTalKsQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se4HHIwzuM6l",
        "colab_type": "text"
      },
      "source": [
        "## 7. Data Augmetation\n",
        "Now, instead of giving the FashionMNIST dataset directly to the network, augment it first using:\n",
        "```\n",
        "keras.preprocessing.image.ImageDataGenerator\n",
        "```\n",
        "Specifically, use vertical and horizontal flips and 20 degrees rotation. We also want to normalise the data. Feel free to consult the documentation for this function.\n",
        "What effect does this have on your model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbnA3Wd_KvCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RujmNUTRmAG",
        "colab_type": "text"
      },
      "source": [
        "## 8. Custom layers\n",
        "In one of the TA sessions, we briefly went over how to implement a custom layer -- specifically, we re-implemented the Dense (or fully-connected) layer. In this part, we will get more practice implementing custom layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjUYKqSjsD_C",
        "colab_type": "text"
      },
      "source": [
        "Please reimplement a simplified version of `tf.keras.layers.Conv2D` using `tf.nn.conv2d`: https://www.tensorflow.org/api_docs/python/tf/nn/conv2d\n",
        "\n",
        "Note carefully the difference between these two tf constructs. In particular, `tf.keras.layers.Conv2D` is a high-level implementation of the 2D convolution that defines all the parameters under the hood, while `tf.nn.conv2d` requires you to define your own convolutional kernels via `tf.Variable` (you may also use the `add_weight` function if you desire). \n",
        "- Your implementation should also include a bias variable, consistent with the default behavior of `tf.keras.layers.Conv2D`.\n",
        "- Your constructor should accept 4 parameters: filters, kernel_size, strides, and activation.\n",
        "- You may hard-code the padding as `'SAME'`\n",
        "- You do not have to implement the string shortcuts for the activations (e.g., if your code handles `tf.nn.relu` but not `'relu'`, that's okay).\n",
        "- For simplicity, you may assume that kernel_size and strides are integers (i.e., as opposed to lists).\n",
        "- Initialize all weights using the standard normal distribution.\n",
        "\n",
        "Note that since you only have to deal with the above 5 input arguments, your implementation will not be as sophisticated as `tf.keras.layers.Conv2D`, which contains many other input arguments. Rather, the point of this exercise is for you to get a better understanding of what tf is doing under the hood so that you are not just blindly using their high-level functions.\n",
        "\n",
        "Feel free to refer to the notebook from the TA session or any online tf documentation, though please do not copy the source code from tf's native implementation of Conv2D.\n",
        "\n",
        "After you're done, repeat the CNN defined above, substituting all instances of `tf.keras.layers.Conv2D` with your implementation, and run for 10 epochs. It's okay if you don't get the same accuracy, but it should still improve. Also print out the `.summary()` command to ensure that the number of parameters is the same as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vxp2tYd8RndZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Conv2D(tf.keras.layers.Layer):\n",
        "  def __init__(self, filters, kernel_size, strides, activation):\n",
        "    super().__init__()\n",
        "    pass\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    # expect the input_shape to be (batch_size, height, width, filters_previous)\n",
        "    pass\n",
        "  \n",
        "  def call(self, input):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-16TCZxsS6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy and paste an earlier training script, and replace the native conv2Ds with yours"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWn7tWEir2Du",
        "colab_type": "text"
      },
      "source": [
        "##** Bonus question: Custom layer for Fourier filtering\n",
        "\n",
        "Note: this problem requires some careful bug-checking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVL9zoNPzaln",
        "colab_type": "text"
      },
      "source": [
        "Now, we will implement a custom layer that doesn't exist in keras -- Fourier filtering. Your layer should apply the 2D Fourier transform (`tf.signal.fft2d`) to each channel of the input, multiply element-wise by an optimizable mask (a different one for each channel), apply the 2D inverse Fourier transform (`tf.signal.ifft2d`), and then take the absolute value. Note: \n",
        "- You will have to use the tf versions of all operations, NOT the numpy versions. \n",
        "- The fft2d operations in tensorflow are done on the LAST two dimensions, which is at odds with the default dimension ordering of CNNs. Thus, you will need to use `tf.transpose` on the input and then transpose back after the filtering operation.\n",
        "- Use dtype `tf.complex64`, which is basically a combination of two `tf.float32`s. You will have to explicitly cast between these two data types, because the input/output will be `tf.float32`, but intermediate steps will be `tf.complex64`.\n",
        "\n",
        "Initialize your optimizable Fourier masks using a binary circular mask (1's inside the circle, 0's outside), with a radius given by 1/4 of the square image dimension (you can round if not divisible by 4). \n",
        "\n",
        "After defining this custom layer, copy your previously defined CNN above and insert this new layer as the first layer. To verify that your layer is working correctly, plot some example outputs of the first layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmqnrPjs0LcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FourierFilter(tf.keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  \n",
        "  def build(self, input_shape):\n",
        "    # expect the input_shape to be (batch_size, height, width, filters_previous)\n",
        "    pass\n",
        "  \n",
        "  def call(self, input):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_T42rvrwaz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}