{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BME_548L_Lab8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zfP8R-_IFtN"
      },
      "source": [
        "# Custom Layers and other neural network modifications\n",
        "\n",
        "When we build neural networks we often use pre-defined layer types which are provided by tensorflow/keras. For example:\n",
        "\n",
        "- Conv2D\n",
        "- Dense\n",
        "- MaxPool2D\n",
        "\n",
        "All of these layers are simply wrapper over a set of operations defined and parameterized in tensorflow.\n",
        "\n",
        "In this lab we will review how these \"fundemental layers\" are created, as well as create a few of our own. Unfortunately for you, we will be avoiding the `Conv2D` layer as that's part of the homework.\n",
        "\n",
        "The beautiful thing about tensorflow/pytorch (and other modern deep learning libraries) is that gradients are handeled for you. So all you need to do is write a differentiable forward-pass, then autograd will take care of the rest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuS_DrymJbYo"
      },
      "source": [
        "## Dense Layer\n",
        "From first principles dense layers are the easiest to understand. They are simply a large matrix multiplication:\n",
        "\n",
        "$$ d_{out} = \\theta_i d_{in} + bias$$\n",
        "\n",
        "Note we don't include the non-linearity in the mathematical definition of the sense layer, but we do in the programatic definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv0eF_PPKoY0"
      },
      "source": [
        "### Dense layer implementation\n",
        "If I wanted to implement a dense layer in Tensorflow I could either a) build it up from first principles, or b) peak at the source code\n",
        "\n",
        "In this lab we will build it up from \"first principles\" (with a little guidance) and then we can compare our implementation to Tensorflows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djZaMP7FLCwV"
      },
      "source": [
        "Before we go definining math, we need to understand what tensorflow/keras is looking for. They define a class called `Layer` for us to inherit from:\n",
        "\n",
        "https://github.com/tensorflow/tensorflow/blob/v2.3.1/tensorflow/python/keras/engine/base_layer.py#L104-L3035\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Bs8v8bIA45"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQalWdA4L3xT"
      },
      "source": [
        "class MyLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      # called during class instantiotion\n",
        "      super(MyLayer, self).__init__()\n",
        "      # store initiliazation variables and create weights\n",
        "\n",
        "    def call(self):\n",
        "        # called during the forward pass of the model\n",
        "        # define forward pass\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piUnoZvDM8-c"
      },
      "source": [
        "### A couple tensorflow basics:\n",
        "\n",
        "**Variables**\n",
        "\n",
        "TF variables are parameters which are used to define our mathematical operations. They have attributes like `trainable`, `initial_value`, `name`, `dtype`, and more. When we create our layers using Tensorflow method we don't need to define these as they are kept inside the layer object.\n",
        "\n",
        "**Linear Algebra**\n",
        "\n",
        "You should hopefully be familiar with the basic linear algebra which we use to create neural networks. For the most part tensorflow will provide these operations directly using an API that makes sense. Here are a few examples:\n",
        "\n",
        "- Matrix multiplication: `tf.matmul`\n",
        "- Identity matrix: `tf.eye`\n",
        "\n",
        "However there are also some specific operations which are more `nn` specific:\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/nn\n",
        "\n",
        "- MaxPool1D: `tf.nn.max_pool1d`\n",
        "- ReLU: `tf.nn.relu`\n",
        "- Softmax: `tf.nn.softmax`\n",
        "\n",
        "Finally there are a *few* operations which are defined by overloading existing operators. For example the `+` operator is defined between two tensors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RrWroz3TVZo"
      },
      "source": [
        "## Exercise 1\n",
        " \n",
        " - Create a Simple Dense layer\n",
        "\n",
        "I've provided the \"scaffolding\" to get you started. In addition here are some useful code fragments:\n",
        "\n",
        "\n",
        "#### Weight Initialization\n",
        "```\n",
        "# random normal\n",
        "weight_init = tf.random_normal_initializer()\n",
        "# zeros\n",
        "weight_init = tf.zeros_initializer()\n",
        "# ones\n",
        "weight_init = tf.ones_initializer()\n",
        "```\n",
        "\n",
        "#### Variable declaration\n",
        "```\n",
        "weight = tf.Variable(initial_value=weight_init(shape=(n,m)), dtype='float32' ,trainable=...)\n",
        "```\n",
        "\n",
        "#### Tensor Manipulation\n",
        "```\n",
        "matmuled = tf.matmul(a,b)\n",
        "added = a + b\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wdan2_EM_qq"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class CustomDense(tf.keras.layers.Layer):\n",
        "    def __init__(self, ...):\n",
        "        super(CustomDense, self).__init__()\n",
        "        ...\n",
        "    \n",
        "\n",
        "    def call(self, inputs):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swTG2-O0WdTe"
      },
      "source": [
        "import numpy as np\n",
        "# code to test your Dense Layer (you don't need to read)\n",
        "# 1. create a fake model that transforms random data into random data\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Input(5), tf.keras.layers.Dense(2)])\n",
        "model.compile(loss='MSE')\n",
        "# 2. create a bunch of fake data\n",
        "fake_x_data = np.random.normal(size=(10000, 5))\n",
        "fake_y_data = model.predict(fake_x_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-Rc28H4YxjX",
        "outputId": "0adde55c-92cd-416c-e98f-7d4d6d7fe577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# now we create a new model that uses your layer (input size of 5, output size of 2)\n",
        "custom_model = tf.keras.models.Sequential([tf.keras.layers.Input(5), CustomDense(...)])\n",
        "custom_model.compile(loss='MSE')\n",
        "hist = custom_model.fit(x=fake_x_data, y=fake_y_data)\n",
        "test_result = hist.history['loss'][0] < 1e-5\n",
        "if test_result:\n",
        "    print(\"Success!\")\n",
        "else:\n",
        "    print(\"Test Failed\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 0s 934us/step - loss: 1.7356e-06\n",
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_E4_ATmZnN_"
      },
      "source": [
        "## Exercise 2: Improving our dense layer\n",
        "\n",
        "### Exercise 2a\n",
        "Our dense layer can be better! We will start with the simple modification of including a non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCDecWU8cG9m"
      },
      "source": [
        "# copy your dense layer code from exercise 1 here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqWTCgR5ZSQk"
      },
      "source": [
        "import numpy as np\n",
        "# code to test your Dense Layer (you don't need to read)\n",
        "# 1. create a fake model that transforms random data into random data\n",
        "test_model = tf.keras.models.Sequential([tf.keras.layers.Input(5), tf.keras.layers.Dense(5, activation='tanh'),  tf.keras.layers.Dense(2, activation='tanh')])\n",
        "test_model.compile(loss='MSE')\n",
        "# 2. create a bunch of fake data\n",
        "fake_x_data = np.random.normal(size=(10000, 5))\n",
        "fake_y_data = test_model.predict(fake_x_data)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6Zd-9leM8mN"
      },
      "source": [
        "# now we create a new model that uses your layer twice [(input size of 5, output size of 5), (input size of 5, output size of 2)\n",
        "custom_model = tf.keras.models.Sequential([tf.keras.layers.Input(5), CustomDense(..., activation='tanh'), CustomDense(..., activation='tanh')])\n",
        "custom_model.compile(loss='MSE')\n",
        "hist = custom_model.fit(x=fake_x_data, y=fake_y_data, epochs=15)\n",
        "test_result = hist.history['loss'][-1] < 0.001\n",
        "if test_result:\n",
        "    print(\"Success!\")\n",
        "else:\n",
        "    print(\"Test Failed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50hFptVucKbL"
      },
      "source": [
        "## Exercise 2b\n",
        "\n",
        "Now you may have noticed that you have to define more things for your layer than the tensorflow implementation does (specifically you need to define both input and output size). This is because tensorflow is using the `build` method.\n",
        "\n",
        "We can look at a more advanced version of our custom layer spec as:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkYI2Lk3bgMF"
      },
      "source": [
        "class MyLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "      # called during class instantiotion\n",
        "      super(MyLayer, self).__init__()\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # store initiliazation variables and create weights\n",
        "        pass\n",
        "\n",
        "    def call(self):\n",
        "        # called during the forward pass of the model\n",
        "        # define forward pass\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmYaNrXXcgVN"
      },
      "source": [
        "The `build` function is a better way to define your weight creation/variable initialization. You can see that the function has a `input_shape` argument. This is the shape of the data from the **previous** layer.\n",
        "\n",
        "The `build` function will be automatically called *right-before* the first forward pass.\n",
        "\n",
        "**Modify your custom layer so you only have to specify the output size of the layer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWjGl0tccfhN"
      },
      "source": [
        "# copy and paste your custom layer here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13t705uZdKUC"
      },
      "source": [
        "import numpy as np\n",
        "# code to test your Dense Layer (you don't need to read)\n",
        "# 1. create a fake model that transforms random data into random data\n",
        "test_model = tf.keras.models.Sequential([tf.keras.layers.Input(5), tf.keras.layers.Dense(5, activation='tanh'),  tf.keras.layers.Dense(2, activation='tanh')])\n",
        "test_model.compile(loss='MSE')\n",
        "# 2. create a bunch of fake data\n",
        "fake_x_data = np.random.normal(size=(10000, 5))\n",
        "fake_y_data = test_model.predict(fake_x_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Cy6gd7dLdq"
      },
      "source": [
        "############################################\n",
        "# You shouldn't need to change this code!!!#\n",
        "############################################\n",
        "custom_model = tf.keras.models.Sequential([tf.keras.layers.Input(5), CustomDense(5, activation='tanh') CustomDense(2, activation='tanh')])\n",
        "custom_model.compile(loss='MSE')\n",
        "hist = custom_model.fit(x=fake_x_data, y=fake_y_data, epochs=15)\n",
        "test_result = hist.history['loss'][-1] < 0.001\n",
        "if test_result:\n",
        "    print(\"Success!\")\n",
        "else:\n",
        "    print(\"Test Failed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6vUTMd-djSA"
      },
      "source": [
        "# Exercise 3 - Compositing Layers\n",
        "\n",
        "**background**:\n",
        "\n",
        "We have seen a couple simple ways to composite layers (functional model declaration and sequential model declaration). We can also use custom layers to group operations/variables together.\n",
        "\n",
        "Autograd (the program which tags all the operations to include in the graph) will automatically pick up on the variables involved in the forward pass. So we can actually define layers within layers.\n",
        "\n",
        "This is particullarly useful when we have a specific design pattern we are using and want to repeat it (think back to the inception architecture and the repetition of the multiple sets of concurrent convolutions).\n",
        "\n",
        "### 3A - Create composite layer\n",
        "\n",
        "We want to create a composite layer that encapsualtes the `conv-conv-pool` design pattern we see in alexnet.\n",
        "\n",
        "You can use existing layers (`conv2d` and `maxpool2d`).\n",
        "\n",
        "*hint*: you don't need to use the build function, why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDNAMm70BEOy"
      },
      "source": [
        "class ConvConvPool(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_filters, activation='relu'):\n",
        "        super().__init__(self)\n",
        "\n",
        "    def call(self):\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUHXjwAfBhJJ"
      },
      "source": [
        "### 3B - Use Composite Layer\n",
        "\n",
        "Test code is provided"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD3bQp2eBkoQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "num_filters_1 = 16\n",
        "num_filters_2 = 32\n",
        "k_size = 3\n",
        "test_model = tf.keras.models.Sequential(layers = [\n",
        "                                                tf.keras.layers.Input(shape=(64, 64, 3)),\n",
        "                                                tf.keras.layers.Conv2D(num_filters_1, kernel_size=k_size, activation='relu'),\n",
        "                                                tf.keras.layers.Conv2D(num_filters_1, kernel_size=k_size, activation='relu'),\n",
        "                                                tf.keras.layers.MaxPool2D(),\n",
        "                                                tf.keras.layers.Conv2D(num_filters_2, kernel_size=k_size, activation='relu'),\n",
        "                                                tf.keras.layers.Conv2D(num_filters_2, kernel_size=k_size, activation='relu'),\n",
        "                                                tf.keras.layers.MaxPool2D(),\n",
        "                                                tf.keras.layers.Flatten(),\n",
        "                                                tf.keras.layers.Dense(1)                        \n",
        "])\n",
        "\n",
        "composite_model = tf.keras.models.Sequential(layers = [\n",
        "                                                tf.keras.layers.Input(shape=(64, 64, 3)),\n",
        "                                                ConvConvPool(num_filters_1, 'relu'),\n",
        "                                                ConvConvPool(num_filters_2, 'relu'),\n",
        "                                                tf.keras.layers.Flatten(),\n",
        "                                                tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPkhr_t9CgqE"
      },
      "source": [
        "import numpy as np\n",
        "test_x_data = np.random.uniform(size=(2500, 64, 64, 3))\n",
        "test_model.compile(loss='MSE')\n",
        "test_y_data = test_model.predict(test_x_data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7NVoUhSCvFh",
        "outputId": "4bd9892e-6bf9-49fc-dc11-e299fae79e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "composite_model.compile(loss='MSE')\n",
        "hist = composite_model.fit(test_x_data, test_y_data, epochs=2)\n",
        "test_result = hist.history['loss'][-1] < 0.001\n",
        "if test_result:\n",
        "    print(\"Success!\")\n",
        "else:\n",
        "    print(\"Test Failed\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "79/79 [==============================] - 16s 206ms/step - loss: 4.3851e-04\n",
            "Epoch 2/2\n",
            "79/79 [==============================] - 16s 206ms/step - loss: 4.5975e-04\n",
            "Success!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}