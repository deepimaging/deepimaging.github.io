{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BME_548L_Lab12.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KF-2cBmu-Bq"
      },
      "source": [
        "# Aside\n",
        "\n",
        "In previous iterations of the class we gave a tutorial on coherent vs. incoherent imaging. That knowledge is still valuable but less suited to an interactive lab. You can find the notebook for that tutorial here:\n",
        "https://deepimaging.github.io/data/Comparing_incoherent_vs_coherent_imaging.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHwr7KMOi6vf"
      },
      "source": [
        "# Part 1 -- Eager TF\n",
        "\n",
        "Throughout these labs we have been working with a single side of tensorflow. That is, the graph based approach (with the fit function). While there is nothing you **can't** do in the graph based version of tensorflow, there are design advantages of using other modes (eager mode) or other frameworks.\n",
        "\n",
        "Personally, I've found in my research the PyTorch is a great alternative to Tensorflow, as it follows a much more pythonic interface and is easier to debug. PyTorch follows a dynamic graph approach, where operations are applied on the fly. This may seem a bit weird at first, but it at the end allows for more flexibility and control.\n",
        "\n",
        "Recently, in tensorflow's 2.0 update, eager mode was added. This mode is similar to PyTorch in that is allows for dynamic graphs and a more flexibile representation of computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eaj43sVoj3gs"
      },
      "source": [
        "# Code Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohhU9NsgkZhd"
      },
      "source": [
        "## Keras (graph based TF)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC08unVxkbFB"
      },
      "source": [
        "from tensorflow import keras\n",
        "# creating a model\n",
        "def create_model(input_shape, num_classes):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    x = inputs\n",
        "    x = keras.layers.Conv2D(16, 3, activation='relu')(x)\n",
        "    x = keras.layers.Conv2D(16, 3, activation='relu')(x)\n",
        "    x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "    x = keras.layers.Dense(num_classes)(x)\n",
        "    return keras.Model(inputs=inputs, outputs=x)\n",
        "    \n",
        "model = create_model((64, 64, 3), 2)\n",
        "\n",
        "# training a model\n",
        "model.fit(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5VRSzH5kbgQ"
      },
      "source": [
        "## Tensorflow Eager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhphIWG7kew6"
      },
      "source": [
        "# creating a model\n",
        "model = keras.Sequential([\n",
        "  keras.layers.Conv2D(16,[3,3], activation='relu',\n",
        "                         input_shape=(None, None, 1)),\n",
        "  keras.layers.Conv2D(16,[3,3], activation='relu'),\n",
        "  keras.layers.GlobalAveragePooling2D(),\n",
        "  keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "# training a model\n",
        "optimizer = keras.optimizers.Adam()\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "for i in range(num_epochs):\n",
        "    for images, labels in dataset:\n",
        "        with tf.GradientTape() as tape:\n",
        "            model_out = model(images)\n",
        "            loss_value = loss_fn(labels, images)\n",
        "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRM8UWV0odux"
      },
      "source": [
        "## So what is going on here?\n",
        "\n",
        "Before we would specify our models graph apriori (keras model) then compile it. This essentially fixes the order of operations and sets up the gradient pathways (we know the function to calculate the gradient at any given point).\n",
        "\n",
        "When using the eager execution we apply the operations and define the gradient pathway simulataneously. This situation consists of a couple key elements:\n",
        "1. The model, or models. We still need to define our model, but now we don't need to define it using a `Model` class. This is because all tensorflow requires is a definition of the operations being performed, and objects which store the parameters for those operations\n",
        "2. The loss function. Now you can see we have to use a loss function object. This will calculate the loss between our targets (labels) and our models output. Before Keras was doing this internally\n",
        "3. The optimizer. Again this should look familiar but a bit different. The optimizer is still the same optimizer we used previously, but now we are calling the optimizer with arguments (gradients + variables)\n",
        "4. Finally the `GradientTape`. This is the most important part. This \"tape\" is tracking the operations as they occur, so that when we want to calculate gradients we can \"trace back\" the loss from the models output to each of the parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geDjMhozrtwt"
      },
      "source": [
        "## Why do this?\n",
        "\n",
        "While this approach may seem to make sense, it sure seems like a lot more work than what we were doing before. There are however a couple advantages to this approach:\n",
        "1. It allows us to be more flexible about how data is processed. You don't strictly need to put all your trainable variables into one model object (see HW). It also means the pathway the gradients follow can be dynamic (you can swap out the model partway through training, or easily share components amoung several models)\n",
        "2. It can be much easier to debug. Since all the components are seperated you can inspect the intermediate state of your variables and examine how your data is being processed.\n",
        "3. You are not restricted to layer objects. Since everything is about tracking gradients, all that is required is that the operations are differentiable. This means that if you are implementing a custom layer, you can do so directly without having to encapsulate it in a class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dl8xuKwqudr5"
      },
      "source": [
        "## Other considerations\n",
        "\n",
        "While this approach may seem very different than what you've been using so far, the changes are actually quite minor. You still use the same functions to generate models (minus the Model and Inputs declarations) and the same principles that we have been building on all semester still apply.\n",
        "\n",
        "What you use is up to you, the increased flexibility of eager mode may not be worth the amount of extra coding you need to do. Additionally there is a computational overhead to eager mode from static mode (what we were doing before), so for large models you'll notice that things train slower."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu0K-kzIvv3u"
      },
      "source": [
        "# Part 2 - Random Distributions and Deep Learning\n",
        "\n",
        "Random distributions are an incredibly useful tool in machine learning. They allow us to work in a more realistic, less deterministic, space. You should all be familiar with the basic distributions, but a quick review of the two most useful:\n",
        "\n",
        "1. Gaussian -- Continuous random normal (bell curve shape)\n",
        "    - Parameterized by a mean and standard deviation (can be multi-dimensional)\n",
        "2. Categorical -- Discrete random, fixed number of outcomes (this encapsultes binomial)\n",
        "    - Parameterized by a per-class probability"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BIUVIspxuZT"
      },
      "source": [
        "## Probability + DL\n",
        "\n",
        "Random distributions are useful within deep learning as we often have situations which are not deterministic, or we want to understand the degree of certainty we have within our model. \n",
        "\n",
        "A great example of this is the popular *Variational AutoEncoder*, which uses a mean and standard deviation at the encoding level instead of a fixed vector.\n",
        "\n",
        "We can also use random distributions within the context of physical layers to define how data is sampled. While a bit too cutting edge for this lab, the newly (2016) developed distrubtions called \"Relaxed Categorical\" distrubitons allow us to simulate discrete sampling using a differentiable discrete distribution.\n",
        "- Aside, why can't we use a normal categorical distribution within our neural network (or as a physical layer)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9UpVEY-wAQ2"
      },
      "source": [
        "## Exercises - VAE\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaHUKP-fy596"
      },
      "source": [
        "We are going to build an autoencoder similar to Lab 11, but this time we will add a variational element to it.\n",
        "\n",
        "What does this mean practically? It means that the embedding layer is now a mean + standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvae9zJM2k3g"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj81R9Nk7PWg"
      },
      "source": [
        "train_size = 60000\n",
        "batch_size = 128\n",
        "test_size = 10000\n",
        "\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
        "                 .shuffle(train_size).batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
        "                .shuffle(test_size).batch(batch_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTZ6TPBo2mhE"
      },
      "source": [
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Input, UpSampling2D, Dense, Reshape, Flatten, InputLayer\n",
        "import tensorflow as tf\n",
        "\n",
        "# defining the two portions of our model\n",
        "\n",
        "latent_dim = 1\n",
        "encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=32, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                filters=64, kernel_size=3, strides=(2, 2), activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            # No activation\n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=7*7*32, activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=64, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=32, kernel_size=3, strides=2, padding='same',\n",
        "                activation='relu'),\n",
        "            # No activation\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                filters=1, kernel_size=3, strides=1, padding='same'),\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxXonPaT4kvx"
      },
      "source": [
        "## Exercise - 1A\n",
        "\n",
        "Define the training loop using TF eager. Much of the code is provided for you, fill in the blanks and try to understand whats going on..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeEYnYgJ8jWb"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2CTKJYY4wJt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "# choose an optimizer\n",
        "optimizer = ...\n",
        "\n",
        "# this is a bit too advanced for this class...calculating the marginal log liklihood\n",
        "# see: https://www.tensorflow.org/tutorials/generative/cvae\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)\n",
        "\n",
        "def compute_loss(enc_mean, enc_log_variance, sampled,  output, labels):\n",
        "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=labels)\n",
        "    logpx_z = -tf.reduce_sum(cross_entropy, axis=[1, 2, 3])\n",
        "    logpz = log_normal_pdf(sampled, 0., 0.)\n",
        "    logqz_x = log_normal_pdf(sampled, mean, logvar)\n",
        "    return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "def sample_encoding(mean, logvar):\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch_num in range(num_epochs):\n",
        "    # remember our images and labels are the same, duplicated for convenience\n",
        "    with tqdm(total=len(dataset)) as pbar:\n",
        "        avg_loss = .0\n",
        "        for step, (images, labels) in enumerate(dataset):\n",
        "            with tf.GradientTape() as tape:\n",
        "                # get the model encoding\n",
        "                encoding = ...\n",
        "                mean, logvar = tf.split(encoding, num_or_size_splits=2, axis=1)\n",
        "                # use the provided function to sample an encoding using the mean + std\n",
        "\n",
        "                # decode the sampled encoding\n",
        "\n",
        "                # use the provided loss function to calculate loss\n",
        "\n",
        "                # calculate gradients using gradient tape\n",
        "                # hint you'll need to do this for BOTH model objects (encoder + decoder)\n",
        "                trainable_variables = ... # hint they are both lists\n",
        "\n",
        "\n",
        "                # apply the gradients using the optimizer\n",
        "\n",
        "                # code provided to log the loss\n",
        "                avg_loss = (avg_loss * step + loss.numpy().mean())/(step + 1)\n",
        "                pbar.set_description(f\"Loss = {avg_loss:.3f}\")\n",
        "                pbar.update()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQn2b8iCTEH"
      },
      "source": [
        "## Exercise 1B - Sample Decoded Images\n",
        "\n",
        "The nice thing about VAEs is that we can sample multiple representations for each image reconstruction. This lets us know which features the model is certain of, and which ones it's not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Lw8jDgCzvv"
      },
      "source": [
        "# step 1. encode a sample image or set of images, it is easy to simply take the most recent set of images from the training set:\n",
        "sample_input_images = images\n",
        "sample_image_encoding = ...\n",
        "# step 2. Use the split code (in previous cell) to get the mean + logvar for these images\n",
        "\n",
        "# step 3. Use the sampling function to generate five independent samples \n",
        "\n",
        "# step 4. Decode the samples using the decoder\n",
        "\n",
        "# step 5. post-process the decoded samples by applying the sigmoid function\n",
        "post_processed = tf.sigmoid(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BvJVk2RDiVY"
      },
      "source": [
        "## Exercise 1C - Visualize the Decoded Images\n",
        "\n",
        "Now that you have the decoded images, (5 samples per image) you can display them to see how they vary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hu-EcLbHDrvD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGj7baQCh3AB"
      },
      "source": [
        "## Exercise 1D - Visualize the Variance between Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9b07PSeh9un"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}